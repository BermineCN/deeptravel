{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Implementation of Pointer networks: http://arxiv.org/pdf/1506.03134v1.pdf.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Implementation of Pointer networks: http://arxiv.org/pdf/1506.03134v1.pdf.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import DataGenerator\n",
    "from pointer import pointer_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('batch_size', 32, 'Batch size.  ')\n",
    "flags.DEFINE_integer('max_steps', 10, 'Number of numbers to sort.  ')\n",
    "flags.DEFINE_integer('rnn_size', 32, 'RNN size.  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointerNetwork(object):\n",
    "    \n",
    "    def __init__(self, max_len, input_size, size, num_layers, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor):\n",
    "        \"\"\"Create the network. A simplified network that handles only sorting.\n",
    "        \n",
    "        Args:\n",
    "            max_len: maximum length of the model.\n",
    "            input_size: size of the inputs data.\n",
    "            size: number of units in each layer of the model.\n",
    "            num_layers: number of layers in the model.\n",
    "            max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "            batch_size: the size of the batches used during training;\n",
    "                the model construction is independent of batch_size, so it can be\n",
    "                changed after initialization if this is convenient, e.g., for decoding.\n",
    "            learning_rate: learning rate to start with.\n",
    "            learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "            \n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.decoder_targets = []\n",
    "        self.target_weights = []\n",
    "        for i in range(max_len):\n",
    "            self.encoder_inputs.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, input_size], name=\"EncoderInput%d\" % i))\n",
    "\n",
    "        for i in range(max_len + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, input_size], name=\"DecoderInput%d\" % i))\n",
    "            self.decoder_targets.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, max_len + 1], name=\"DecoderTarget%d\" % i))  # one hot\n",
    "            self.target_weights.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, 1], name=\"TargetWeight%d\" % i))\n",
    "\n",
    "            \n",
    "        # Encoder\n",
    "        \n",
    "        # Need for attention\n",
    "        encoder_outputs, final_state = tf.nn.rnn(cell, self.encoder_inputs, dtype = tf.float32)\n",
    "        \n",
    "        # Need a dummy output to point on it. End of decoding.\n",
    "        encoder_outputs = [tf.zeros([FLAGS.batch_size, FLAGS.rnn_size])] + encoder_outputs\n",
    "\n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        top_states = [tf.reshape(e, [-1, 1, cell.output_size])\n",
    "                      for e in encoder_outputs]\n",
    "        attention_states = tf.concat(1, top_states)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            outputs, states, _ = pointer_decoder(\n",
    "                self.decoder_inputs, final_state, attention_states, cell)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\", reuse=True):\n",
    "            predictions, _, inps = pointer_decoder(\n",
    "                self.decoder_inputs, final_state, attention_states, cell, feed_prev=True)\n",
    "            \n",
    "        self.predictions = predictions\n",
    "\n",
    "        self.outputs = outputs\n",
    "        self.inps = inps\n",
    "        # move code below to a separate function as in TF examples\n",
    "        \n",
    "            \n",
    "    def create_feed_dict(self, encoder_input_data, decoder_input_data, decoder_target_data):\n",
    "        feed_dict = {}\n",
    "        for placeholder, data in zip(self.encoder_inputs, encoder_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder, data in zip(self.decoder_inputs, decoder_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder, data in zip(self.decoder_targets, decoder_target_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder in self.target_weights:\n",
    "            feed_dict[placeholder] = np.ones([self.batch_size, 1])\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        loss = 0.0\n",
    "        for output, target, weight in zip(self.outputs, self.decoder_targets, self.target_weights):\n",
    "            loss += tf.nn.softmax_cross_entropy_with_logits(output, target) * weight\n",
    "\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        test_loss = 0.0\n",
    "        for output, target, weight in zip(self.predictions, self.decoder_targets, self.target_weights):\n",
    "            test_loss += tf.nn.softmax_cross_entropy_with_logits(output, target) * weight\n",
    "\n",
    "        test_loss = tf.reduce_mean(test_loss)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        train_loss_value = 0.0\n",
    "        test_loss_value = 0.0\n",
    "        \n",
    "        correct_order = 0\n",
    "        all_order = 0\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            merged = tf.merge_all_summaries()\n",
    "            writer = tf.train.SummaryWriter(\"/tmp/pointer_logs\", sess.graph)\n",
    "            init = tf.initialize_all_variables()\n",
    "            sess.run(init)\n",
    "            for i in range(10000):\n",
    "                encoder_input_data, decoder_input_data, targets_data = dataset.next_batch(\n",
    "                    FLAGS.batch_size, FLAGS.max_steps)\n",
    "\n",
    "                # Train\n",
    "                feed_dict = self.create_feed_dict(\n",
    "                    encoder_input_data, decoder_input_data, targets_data)\n",
    "                d_x, l = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                train_loss_value = 0.9 * train_loss_value + 0.1 * d_x\n",
    "                                \n",
    "                if i % 100 == 0:\n",
    "                    print('Step: %d' % i)\n",
    "                    print(\"Train: \", train_loss_value)\n",
    "\n",
    "                encoder_input_data, decoder_input_data, targets_data = dataset.next_batch(\n",
    "                    FLAGS.batch_size, FLAGS.max_steps, train_mode=False)\n",
    "                # Test\n",
    "                feed_dict = self.create_feed_dict(\n",
    "                    encoder_input_data, decoder_input_data, targets_data)\n",
    "                inps_ = sess.run(self.inps, feed_dict=feed_dict)\n",
    "\n",
    "                predictions = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "                \n",
    "                test_loss_value = 0.9 * test_loss_value + 0.1 * sess.run(test_loss, feed_dict=feed_dict)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Test: \", test_loss_value)\n",
    "\n",
    "                predictions_order = np.concatenate([np.expand_dims(prediction , 0) for prediction in predictions])\n",
    "                predictions_order = np.argmax(predictions_order, 2).transpose(1, 0)[:,0:FLAGS.max_steps]\n",
    "                    \n",
    "                input_order = np.concatenate([np.expand_dims(encoder_input_data_ , 0) for encoder_input_data_ in encoder_input_data])\n",
    "                input_order = np.argsort(input_order, 0).squeeze().transpose(1, 0)+1\n",
    "                \n",
    "                correct_order += np.sum(np.all(predictions_order == input_order,\n",
    "                                    axis=1))\n",
    "                all_order += FLAGS.batch_size\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print('Correct order / All order: %f' % (correct_order / all_order))\n",
    "                    correct_order = 0\n",
    "                    all_order = 0\n",
    "                    \n",
    "                    # print(encoder_input_data, decoder_input_data, targets_data)\n",
    "                    # print(inps_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Train:  2.68860645294\n",
      "Test:  2.6680606842\n",
      "Correct order / All order: 0.000000\n",
      "Step: 100\n",
      "Train:  24.2560872819\n",
      "Test:  24.3124202368\n",
      "Correct order / All order: 0.000000\n",
      "Step: 200\n",
      "Train:  18.7171205404\n",
      "Test:  21.7182790658\n",
      "Correct order / All order: 0.000000\n",
      "Step: 300\n",
      "Train:  15.3441392092\n",
      "Test:  19.2297186092\n",
      "Correct order / All order: 0.000000\n",
      "Step: 400\n",
      "Train:  12.5740649359\n",
      "Test:  16.7632974299\n",
      "Correct order / All order: 0.000000\n",
      "Step: 500\n",
      "Train:  11.2075068048\n",
      "Test:  15.2956578262\n",
      "Correct order / All order: 0.000313\n",
      "Step: 600\n",
      "Train:  10.5922972077\n",
      "Test:  14.4025315836\n",
      "Correct order / All order: 0.000000\n",
      "Step: 700\n",
      "Train:  10.1038848763\n",
      "Test:  14.0646507611\n",
      "Correct order / All order: 0.001250\n",
      "Step: 800\n",
      "Train:  9.84272010742\n",
      "Test:  13.1089141098\n",
      "Correct order / All order: 0.002188\n",
      "Step: 900\n",
      "Train:  9.43882486992\n",
      "Test:  13.2268964085\n",
      "Correct order / All order: 0.002812\n",
      "Step: 1000\n",
      "Train:  9.30646132971\n",
      "Test:  11.9487612554\n",
      "Correct order / All order: 0.008438\n",
      "Step: 1100\n",
      "Train:  9.08544796151\n",
      "Test:  11.494893281\n",
      "Correct order / All order: 0.011250\n",
      "Step: 1200\n",
      "Train:  8.98202517612\n",
      "Test:  10.6009793126\n",
      "Correct order / All order: 0.009687\n",
      "Step: 1300\n",
      "Train:  9.15441774497\n",
      "Test:  10.3164366176\n",
      "Correct order / All order: 0.011875\n",
      "Step: 1400\n",
      "Train:  8.68220538957\n",
      "Test:  10.4113698192\n",
      "Correct order / All order: 0.008125\n",
      "Step: 1500\n",
      "Train:  8.47421340504\n",
      "Test:  9.98926159032\n",
      "Correct order / All order: 0.013437\n",
      "Step: 1600\n",
      "Train:  7.89995591094\n",
      "Test:  9.00162249094\n",
      "Correct order / All order: 0.014063\n",
      "Step: 1700\n",
      "Train:  7.40639584213\n",
      "Test:  8.57478098557\n",
      "Correct order / All order: 0.016250\n",
      "Step: 1800\n",
      "Train:  7.35974999456\n",
      "Test:  8.06004969258\n",
      "Correct order / All order: 0.018437\n",
      "Step: 1900\n",
      "Train:  7.26916386306\n",
      "Test:  8.1215957292\n",
      "Correct order / All order: 0.025313\n",
      "Step: 2000\n",
      "Train:  6.7959001287\n",
      "Test:  7.64490160309\n",
      "Correct order / All order: 0.033125\n",
      "Step: 2100\n",
      "Train:  6.87674831081\n",
      "Test:  7.15278097724\n",
      "Correct order / All order: 0.049688\n",
      "Step: 2200\n",
      "Train:  6.73170580988\n",
      "Test:  7.12994621393\n",
      "Correct order / All order: 0.047500\n",
      "Step: 2300\n",
      "Train:  6.65109677845\n",
      "Test:  7.00724942117\n",
      "Correct order / All order: 0.056250\n",
      "Step: 2400\n",
      "Train:  6.45940203109\n",
      "Test:  6.81805318717\n",
      "Correct order / All order: 0.058125\n",
      "Step: 2500\n",
      "Train:  6.6453763696\n",
      "Test:  7.00775308656\n",
      "Correct order / All order: 0.066562\n",
      "Step: 2600\n",
      "Train:  6.46907251895\n",
      "Test:  6.64360032285\n",
      "Correct order / All order: 0.061562\n",
      "Step: 2700\n",
      "Train:  6.24324518548\n",
      "Test:  6.84134361021\n",
      "Correct order / All order: 0.067812\n",
      "Step: 2800\n",
      "Train:  6.35581651841\n",
      "Test:  6.65117508098\n",
      "Correct order / All order: 0.060000\n",
      "Step: 2900\n",
      "Train:  6.40647208568\n",
      "Test:  6.54786750007\n",
      "Correct order / All order: 0.070000\n",
      "Step: 3000\n",
      "Train:  6.14224936921\n",
      "Test:  6.43034832363\n",
      "Correct order / All order: 0.072188\n",
      "Step: 3100\n",
      "Train:  6.08961001664\n",
      "Test:  6.34171326135\n",
      "Correct order / All order: 0.076250\n",
      "Step: 3200\n",
      "Train:  5.81036254127\n",
      "Test:  6.30744260045\n",
      "Correct order / All order: 0.081250\n",
      "Step: 3300\n",
      "Train:  5.93716139789\n",
      "Test:  6.42967964965\n",
      "Correct order / All order: 0.075000\n",
      "Step: 3400\n",
      "Train:  5.80593719235\n",
      "Test:  6.13296594671\n",
      "Correct order / All order: 0.081562\n",
      "Step: 3500\n",
      "Train:  5.86371382958\n",
      "Test:  6.33471407806\n",
      "Correct order / All order: 0.091250\n",
      "Step: 3600\n",
      "Train:  5.80422361775\n",
      "Test:  5.97553471037\n",
      "Correct order / All order: 0.085938\n",
      "Step: 3700\n",
      "Train:  5.75897166431\n",
      "Test:  6.37538878742\n",
      "Correct order / All order: 0.091875\n",
      "Step: 3800\n",
      "Train:  5.77894732082\n",
      "Test:  6.14098334237\n",
      "Correct order / All order: 0.087813\n",
      "Step: 3900\n",
      "Train:  5.66988020683\n",
      "Test:  6.06982477426\n",
      "Correct order / All order: 0.089688\n",
      "Step: 4000\n",
      "Train:  5.70214303194\n",
      "Test:  6.23250277066\n",
      "Correct order / All order: 0.088750\n",
      "Step: 4100\n",
      "Train:  5.36141672317\n",
      "Test:  6.22978959881\n",
      "Correct order / All order: 0.090000\n",
      "Step: 4200\n",
      "Train:  5.62180035\n",
      "Test:  6.41798575639\n",
      "Correct order / All order: 0.091563\n",
      "Step: 4300\n",
      "Train:  5.72695070945\n",
      "Test:  6.79170014103\n",
      "Correct order / All order: 0.088125\n",
      "Step: 4400\n",
      "Train:  5.5645600785\n",
      "Test:  6.05535433931\n",
      "Correct order / All order: 0.096875\n",
      "Step: 4500\n",
      "Train:  5.78387900664\n",
      "Test:  6.16510595788\n",
      "Correct order / All order: 0.088438\n",
      "Step: 4600\n",
      "Train:  5.51845914252\n",
      "Test:  6.42405931776\n",
      "Correct order / All order: 0.075625\n",
      "Step: 4700\n",
      "Train:  5.49293601961\n",
      "Test:  6.33914043112\n",
      "Correct order / All order: 0.077813\n",
      "Step: 4800\n",
      "Train:  5.36139198749\n",
      "Test:  6.15636217263\n",
      "Correct order / All order: 0.078750\n",
      "Step: 4900\n",
      "Train:  5.62922656542\n",
      "Test:  6.16224490999\n",
      "Correct order / All order: 0.088125\n",
      "Step: 5000\n",
      "Train:  5.40432320323\n",
      "Test:  6.21157728666\n",
      "Correct order / All order: 0.073750\n",
      "Step: 5100\n",
      "Train:  5.24901713876\n",
      "Test:  6.26941818894\n",
      "Correct order / All order: 0.083125\n",
      "Step: 5200\n",
      "Train:  5.26376816275\n",
      "Test:  6.07903759294\n",
      "Correct order / All order: 0.082187\n",
      "Step: 5300\n",
      "Train:  5.35158186651\n",
      "Test:  6.39531300411\n",
      "Correct order / All order: 0.070625\n",
      "Step: 5400\n",
      "Train:  5.13447321112\n",
      "Test:  6.58624444271\n",
      "Correct order / All order: 0.080000\n",
      "Step: 5500\n",
      "Train:  5.25366610842\n",
      "Test:  6.71965085961\n",
      "Correct order / All order: 0.076250\n",
      "Step: 5600\n",
      "Train:  4.96032987994\n",
      "Test:  6.06799493589\n",
      "Correct order / All order: 0.075313\n",
      "Step: 5700\n",
      "Train:  5.12379562302\n",
      "Test:  6.65905823918\n",
      "Correct order / All order: 0.075000\n",
      "Step: 5800\n",
      "Train:  4.74331203995\n",
      "Test:  6.60461442384\n",
      "Correct order / All order: 0.067187\n",
      "Step: 5900\n",
      "Train:  4.86077896205\n",
      "Test:  6.23641909194\n",
      "Correct order / All order: 0.075000\n",
      "Step: 6000\n",
      "Train:  4.78373441585\n",
      "Test:  6.27460395839\n",
      "Correct order / All order: 0.072188\n",
      "Step: 6100\n",
      "Train:  4.83201869486\n",
      "Test:  6.37543079886\n",
      "Correct order / All order: 0.070625\n",
      "Step: 6200\n",
      "Train:  4.81478544301\n",
      "Test:  6.47438060855\n",
      "Correct order / All order: 0.073438\n",
      "Step: 6300\n",
      "Train:  4.64719894561\n",
      "Test:  5.96938197853\n",
      "Correct order / All order: 0.070312\n",
      "Step: 6400\n",
      "Train:  4.55831595262\n",
      "Test:  6.16875777721\n",
      "Correct order / All order: 0.069062\n",
      "Step: 6500\n",
      "Train:  4.48855033001\n",
      "Test:  5.98386695609\n",
      "Correct order / All order: 0.080000\n",
      "Step: 6600\n",
      "Train:  4.59912507904\n",
      "Test:  5.79413920175\n",
      "Correct order / All order: 0.080937\n",
      "Step: 6700\n",
      "Train:  4.38672997257\n",
      "Test:  6.16387904465\n",
      "Correct order / All order: 0.075938\n",
      "Step: 6800\n",
      "Train:  4.26643317556\n",
      "Test:  5.95012128017\n",
      "Correct order / All order: 0.079062\n",
      "Step: 6900\n",
      "Train:  4.37620956494\n",
      "Test:  5.88486577497\n",
      "Correct order / All order: 0.081562\n",
      "Step: 7000\n",
      "Train:  4.11695713485\n",
      "Test:  5.64037828306\n",
      "Correct order / All order: 0.095000\n",
      "Step: 7100\n",
      "Train:  4.17994623282\n",
      "Test:  5.70522660797\n",
      "Correct order / All order: 0.090938\n",
      "Step: 7200\n",
      "Train:  4.08039658243\n",
      "Test:  5.5161641241\n",
      "Correct order / All order: 0.099062\n",
      "Step: 7300\n",
      "Train:  3.98476950896\n",
      "Test:  5.34851756121\n",
      "Correct order / All order: 0.099062\n",
      "Step: 7400\n",
      "Train:  3.73535418302\n",
      "Test:  5.74838267964\n",
      "Correct order / All order: 0.088750\n",
      "Step: 7500\n",
      "Train:  3.93397139072\n",
      "Test:  5.30779510832\n",
      "Correct order / All order: 0.090938\n",
      "Step: 7600\n",
      "Train:  3.95586119262\n",
      "Test:  5.43528508704\n",
      "Correct order / All order: 0.097187\n",
      "Step: 7700\n",
      "Train:  3.91302135311\n",
      "Test:  5.53763298495\n",
      "Correct order / All order: 0.097812\n",
      "Step: 7800\n",
      "Train:  4.18073052533\n",
      "Test:  5.58508930977\n",
      "Correct order / All order: 0.095625\n",
      "Step: 7900\n",
      "Train:  3.96941263808\n",
      "Test:  5.58711305045\n",
      "Correct order / All order: 0.098437\n",
      "Step: 8000\n",
      "Train:  3.85481565563\n",
      "Test:  5.52481524791\n",
      "Correct order / All order: 0.098125\n",
      "Step: 8100\n",
      "Train:  3.87347794306\n",
      "Test:  5.58058721789\n",
      "Correct order / All order: 0.102813\n",
      "Step: 8200\n",
      "Train:  3.95910527306\n",
      "Test:  5.76155282005\n",
      "Correct order / All order: 0.094687\n",
      "Step: 8300\n",
      "Train:  3.85065968524\n",
      "Test:  5.92102639964\n",
      "Correct order / All order: 0.089063\n",
      "Step: 8400\n",
      "Train:  3.67661865337\n",
      "Test:  5.48509328145\n",
      "Correct order / All order: 0.101562\n",
      "Step: 8500\n",
      "Train:  3.74551649664\n",
      "Test:  4.9346951683\n",
      "Correct order / All order: 0.097187\n",
      "Step: 8600\n",
      "Train:  3.72656233211\n",
      "Test:  5.47226592987\n",
      "Correct order / All order: 0.097812\n",
      "Step: 8700\n",
      "Train:  3.67415227498\n",
      "Test:  5.19853890877\n",
      "Correct order / All order: 0.104063\n",
      "Step: 8800\n",
      "Train:  3.62641231406\n",
      "Test:  5.18646203768\n",
      "Correct order / All order: 0.102188\n",
      "Step: 8900\n",
      "Train:  3.69807504126\n",
      "Test:  5.06300161369\n",
      "Correct order / All order: 0.100937\n",
      "Step: 9000\n",
      "Train:  3.47705514477\n",
      "Test:  5.2515885899\n",
      "Correct order / All order: 0.105000\n",
      "Step: 9100\n",
      "Train:  3.64779143786\n",
      "Test:  5.21448380673\n",
      "Correct order / All order: 0.115000\n",
      "Step: 9200\n",
      "Train:  3.53188391085\n",
      "Test:  5.17584119947\n",
      "Correct order / All order: 0.106250\n",
      "Step: 9300\n",
      "Train:  3.54036425297\n",
      "Test:  5.05067381854\n",
      "Correct order / All order: 0.117813\n",
      "Step: 9400\n",
      "Train:  3.57943025998\n",
      "Test:  5.0511636571\n",
      "Correct order / All order: 0.107500\n",
      "Step: 9500\n",
      "Train:  3.64434532467\n",
      "Test:  5.16379682471\n",
      "Correct order / All order: 0.105313\n",
      "Step: 9600\n",
      "Train:  3.5170911503\n",
      "Test:  4.8098224241\n",
      "Correct order / All order: 0.109375\n",
      "Step: 9700\n",
      "Train:  3.46614656961\n",
      "Test:  4.91978658513\n",
      "Correct order / All order: 0.116250\n",
      "Step: 9800\n",
      "Train:  3.56308987133\n",
      "Test:  4.70519466716\n",
      "Correct order / All order: 0.126562\n",
      "Step: 9900\n",
      "Train:  3.59348826346\n",
      "Test:  4.80465774899\n",
      "Correct order / All order: 0.126250\n"
     ]
    }
   ],
   "source": [
    "pointer_network = PointerNetwork(FLAGS.max_steps, 1, FLAGS.rnn_size, 1, 5, FLAGS.batch_size, 1e-2, 0.95)\n",
    "dataset = DataGenerator()\n",
    "pointer_network.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
